# Generative Domain Adaptation for Remote Sensing Object Detection

![Project Banner](path/to/your/generated_image.png)

This repository contains the official datasets and information for the paper: **"Generative Domain Adaptation for Object Detection in Remote Sensing Imagery through Automated Feature-Driven Exemplar In-painting."**

Our work introduces a novel end-to-end pipeline for generating high-fidelity synthetic data to address the common challenges of data scarcity, geographic bias, and limited background diversity in remote sensing object detection. By leveraging a Generative Adversarial Network (GAN) and an intelligent in-painting strategy, we create a powerful pre-training corpus that significantly enhances the generalization and performance of modern object detection models.


## ðŸ“‚ The MEA10K Dataset Collection

We provide a collection of datasets designed to facilitate new research in generative modeling, domain adaptation, and robust object detection in the remote sensing domain.

### 1. **MEA10K: The Complete Synthetic Object Detection Dataset**
This is the primary dataset used for the domain adaptation experiments in our paper. It features photorealistic, GAN-generated backgrounds populated with a diverse set of stochastically placed object exemplars.

*   **Content:** 10,000 synthetic images with object annotations.
*   **Resolution:** 1024x1024 pixels.
*   **Object Classes:** 19 distinct categories.
*   **Annotations:** Provided in YOLO (`.txt`) format. Each annotation file contains one line per object with the format: `[class_id] [x_center_norm] [y_center_norm] [width_norm] [height_norm]`. A `classes.txt` file is included for class name mapping.
*   **Download Link:** `[Link to MEA10K on Google Drive, Zenodo, Hugging Face, etc.]`

### *Optional Datasets*
The following datasets represent the core components used to create MEA10K. They are released to support more specialized research in generative modeling and data synthesis.

### 2. **SYN-BG-10K: Synthetic Generated Backgrounds**
This dataset contains the 10,000 pure background images generated by our GAN, without any objects. It is ideal for researchers wishing to create their own synthetic compositions.

*   **Content:** 10,000 synthetic background images.
*   **Resolution:** 1024x1024 pixels.
*   **Source:** Generated by our GAN.
*   **Download Link:** `[Link to SYN-BG-10K, or state "Available upon request"]`

### 3. **RS-BG-30K: Real-World Global Backgrounds**
This is the foundational dataset of real-world imagery used to train our background generation GAN.

*   **Content:** 30,224 real-world satellite images.
*   **Resolution:** 1024x1024 pixels.
*   **Source:** Google Earth.
*   **Key Feature:** Each image is geospatially referenced with latitude and longitude coordinates.
*   **Download Link:** `[Link to RS-BG-30K, or state "Available upon request due to size/licensing"]`

---

## ðŸš€ Pipeline Overview

The MEA10K dataset was created using a two-stage pipeline:

1.  **Stage 1: Background Synthesis:** We trained a Generative Adversarial Network (GAN) on the `RS-BG-30K` dataset, a massive collection of globally-sourced satellite images. This model learned the underlying distribution of real-world overhead imagery and was used to generate the `SYN-BG-10K` dataset.

2.  **Stage 2: Multi-Model Exemplar Selection and In-painting:** We developed an automated method to select a pool of four distinct representative exemplars for each of our 19 classes. Each exemplar was identified by a different 'specialist' vision model (ResNet, ViT, DINOv2, and CLIP) based on its unique feature embeddings. These exemplars were then subjected to extensive stochastic augmentations (rotation, scale, color) and pasted context-agnostically onto the synthetic backgrounds. For each object placed in an image, an exemplar was randomly chosen from its class-specific pool of four, ensuring high visual diversity in the final `MEA10K` dataset.



![Pipeline Diagram](path/to/your/pipeline_diagram.png)


---

## ðŸ“„ Citation

If you use our datasets or find our work helpful in your research, please cite our paper:

```bibtex
@article{aghili2025generative,
  title={Generative Domain Adaptation for Object Detection in Remote Sensing Imagery through Automated Feature-Driven Exemplar In-painting},
  author={Aghili, Mohamad Ebrahim and Ghassemian, Hassan and Imani, Maryam},
  journal={Journal Name},
  volume={XX},
  number={Y},
  pages={ZZZ--ZZZ},
  year={2025},
  publisher={Publisher}
}
