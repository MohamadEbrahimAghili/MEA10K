# Generative Domain Adaptation for Remote Sensing Object Detection

![BN4](https://github.com/user-attachments/assets/a9456989-b568-46b7-82bd-2678154345c9)



<p align="justify">
This repository contains the official datasets and information for the paper: <strong>"Generative Domain Adaptation for Object Detection in Remote Sensing Imagery through Automated Feature-Driven Exemplar In-painting."</strong>
</p>

<p align="justify">
Our work introduces a novel end-to-end pipeline for generating high-fidelity synthetic data to address the common challenges of data scarcity, geographic bias, and limited background diversity in remote sensing object detection. By leveraging a Generative Adversarial Network (GAN) and an intelligent in-painting strategy, we create a powerful pre-training corpus that significantly enhances the generalization and performance of modern object detection models.
</p>

## ðŸ“‚ The MEA10K Dataset Collection

<p align="justify">
We provide a collection of datasets designed to facilitate new research in generative modeling, domain adaptation, and robust object detection in the remote sensing domain.
</p>

### 1. **MEA10K: The Complete Synthetic Object Detection Dataset**
<p align="justify">
This is the primary dataset used for the domain adaptation experiments in our paper. It features photorealistic, GAN-generated backgrounds populated with a diverse set of stochastically placed object exemplars.
</p>

*   **Content:** 10,000 synthetic images with object annotations.
*   **Resolution:** 1024x1024 pixels.
*   **Object Classes:** 19 distinct categories.
*   **Annotations:** Provided in YOLO (<code>.txt</code>) format. Each annotation file contains one line per object with the format: <code>[class_id] [x_center_norm] [y_center_norm] [width_norm] [height_norm]</code>. A <code>classes.txt</code> file is included for class name mapping.
*   **Download Link:** `[Link to MEA10K on Google Drive, Zenodo, Hugging Face, etc.]`

### *Optional Datasets*
<p align="justify">
The following datasets represent the core components used to create MEA10K. They are released to support more specialized research in generative modeling and data synthesis.
</p>

### 2. **SYN-BG-10K: Synthetic Generated Backgrounds**
<p align="justify">
This dataset contains the 10,000 pure background images generated by our GAN, without any objects. It is ideal for researchers wishing to create their own synthetic compositions.
</p>

*   **Content:** 10,000 synthetic background images.
*   **Resolution:** 1024x1024 pixels.
*   **Source:** Generated by our GAN.
*   **Download Link:** `[Link to SYN-BG-10K, or state "Available upon request"]`

### 3. **RS-BG-30K: Real-World Global Backgrounds**
<p align="justify">
This is the foundational dataset of real-world imagery used to train our background generation GAN.
</p>

*   **Content:** 30,224 real-world satellite images.
*   **Resolution:** 1024x1024 pixels.
*   **Source:** Google Earth.
*   **Key Feature:** Each image is geospatially referenced with latitude and longitude coordinates.
*   **Download Link:** `[Link to RS-BG-30K, or state "Available upon request due to size/licensing"]`

---

## ðŸš€ Pipeline Overview

<p align="justify">
The MEA10K dataset was created using a two-stage pipeline:
</p>

<ol>
  <li>
    <p align="justify">
      <strong>Stage 1: Background Synthesis:</strong> We trained a Generative Adversarial Network (GAN) on the <code>RS-BG-30K</code> dataset, a massive collection of globally-sourced satellite images. This model learned the underlying distribution of real-world overhead imagery and was used to generate the <code>SYN-BG-10K</code> dataset.
    </p>
  </li>
  <li>
    <p align="justify">
      <strong>Stage 2: Multi-Model Exemplar Selection and In-painting:</strong> We developed an automated method to select a pool of four distinct representative exemplars for each of our 19 classes. Each exemplar was identified by a different 'specialist' vision model (ResNet, ViT, DINOv2, and CLIP) based on its unique feature embeddings. These exemplars were then subjected to extensive stochastic augmentations (rotation, scale, color) and pasted context-agnostically onto the synthetic backgrounds. For each object placed in an image, an exemplar was randomly chosen from its class-specific pool of four, ensuring high visual diversity in the final <code>MEA10K</code> dataset.
    </p>
  </li>
</ol>

![Pipeline Diagram](path/to/your/pipeline_diagram.png)


---

## ðŸ“„ Citation

<p align="justify">
If you use our datasets or find our work helpful in your research, please cite our paper:
</p>

```bibtex
@article{aghili2025generative,
  title={Generative Domain Adaptation for Object Detection in Remote Sensing Imagery through Automated Feature-Driven Exemplar In-painting},
  author={Aghili, Mohamad Ebrahim and Ghassemian, Hassan and Imani, Maryam},
  journal={Journal Name},
  volume={XX},
  number={Y},
  pages={ZZZ--ZZZ},
  year={2025},
  publisher={Publisher}
}
